# 🧠 Backpropagation & Gradient Descent from Scratch

This project is a simple implementation of a **feedforward neural network** with **backpropagation** and **gradient descent** — all built from scratch using plain Python and NumPy (optional).

## 🚀 Why I Built This

Understanding how neural networks learn is essential for mastering machine learning. Instead of relying on high-level libraries like TensorFlow or PyTorch, I decided to build the core logic myself to gain a deep understanding of:

- Forward propagation
- Loss computation (MSE)
- Backpropagation (manual gradient calculation)
- Weight and bias updates using gradient descent

## 🛠️ Features

- Single hidden-layer neural network (can be extended)
- Manual forward and backward passes
- Mean Squared Error (MSE) loss
- Weight and bias updates using basic gradient descent
- No external ML frameworks (only standard Python or NumPy)



